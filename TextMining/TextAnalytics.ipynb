{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining\n",
    "\n",
    "### L1. Natural Language Processing With Python Tokenizing words and Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. tokenizing -> Word tokenizers and Sentence tokenizers\n",
    "2. Lexicon and Corporas\n",
    "3. Corpora - > Body of the text, e.g : Medical journal, English language, Director Speach\n",
    "4. lexicon : Words and their means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample text to which i want to tokenize\n",
    "sample_text = \"Text mining, also referred to as text data mining, roughly equivalent to text analytics, is the process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_word = word_tokenize(sample_text)\n",
    "print(token_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_pos = nltk.pos_tag(token_word)\n",
    "tag_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_sntce = sent_tokenize(sample_text)\n",
    "token_sntce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 : Stop words  : Remove those word which has no meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the English Library stopwords\n",
    "stop_word = set(stopwords.words(\"English\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the all from the sample text\n",
    "token_word = word_tokenize(sample_text)\n",
    "print(token_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filtered out the stopwords and store in a list\n",
    "filtered_token = []       # Define a list\n",
    "\n",
    "for w in token_word:\n",
    "    if w not in stop_word:\n",
    "        filtered_token.append(w)\n",
    "print(filtered_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a list of custom stopwords\n",
    "stopwords = ['to', 'as', 'is','of','and','the',',','.', 'Who','am']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removed the stopwords and then the words in a list\n",
    "filteredwords = []\n",
    "for f in token_word:\n",
    "    if f not in stopwords:\n",
    "        filteredwords.append(f)\n",
    "print(filteredwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to tag part-of-speech \n",
    "tagg_pos = nltk.pos_tag(filteredwords)\n",
    "tagg_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L3 : Stemming \n",
    "\n",
    "Take the word and find out the root word,\n",
    "Like : Reading -> Read\n",
    "\n",
    "Bothe the below lines have the same meaning\n",
    "1. I was taking a ride in the car.\n",
    "2. I was riding in the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exampl_words = [\"Python\",\"Pythoning\",\"Pythonly\",\"Pythoned\",\"Pythoner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_text = \"It is very importent to be Pythonly while you are pythoning with python. All pythoner have pythoned poorly.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = word_tokenize(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Steming the each word for this sentance\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Part of speech tagging \n",
    "The process of classifying words into their **part-of-speech** and labeling them accordingly is known as **part-of-speech tagging** or **POS tagging**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to tag the part of speech tokenized\n",
    "\n",
    "def partofspeech():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "partofspeech()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_train = 'Hi, Mr. N. K. Sahoo, How are you? What are you doing now a days? This is good time for Python Programing.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenword = nltk.word_tokenize(text_train)\n",
    "pos_train = nltk.pos_tag(tokenword, tagset='universal')\n",
    "pos_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentence tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Chunking : Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to tag the part of speech tokenized\n",
    "\n",
    "def chunk():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "chunk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 6. Chinking : Chinking from the Chunk\n",
    "\n",
    "Chinking : remove from the chunk, we are chunking all, except chink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_chink():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r\"\"\"{<.*> |}\n",
    "                            }<VB.?|IN|DT>+{\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chinked = chunkParser.parse(tagged)\n",
    "            \n",
    "            chinked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "process_chink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 7. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def namedEnt():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            namedEnt.draw()\n",
    "            \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "namedEnt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 8.  Lemmatizing : Some kind of Steming, gives Synonims or the same word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import  WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lammatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(lammatizer.lemmatize(\"cats\"))\n",
    "print(lammatizer.lemmatize(\"cacti\"))\n",
    "print(lammatizer.lemmatize(\"geese\"))\n",
    "print(lammatizer.lemmatize(\"rocks\"))\n",
    "print(lammatizer.lemmatize(\"better\", pos='a'))\n",
    "print(lammatizer.lemmatize(\"best\", pos='a'))\n",
    "print(lammatizer.lemmatize(\"run\"))\n",
    "print(lammatizer.lemmatize(\"run\", 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 9. Corpora :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = gutenberg.raw('austen-emma.txt')\n",
    "token = sent_tokenize(sample)\n",
    "print(token[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. WordNet : Take word and look up synonims, antinames, definition and even context of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synom = wordnet.synsets(\"Program\")\n",
    "print(synom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synoms = wordnet.synsets(\"Revoked\")\n",
    "print(synoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Synset\n",
    "print(synom[0].name())\n",
    "\n",
    "#just the word\n",
    "print(synom[0].lemmas()[0].name())\n",
    "\n",
    "# Definition\n",
    "print(synom[0].definition())\n",
    "\n",
    "# Examples\n",
    "print(synom[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the synonims of Revoke\n",
    "print(synoms[0].name())\n",
    "\n",
    "# Just synonim just word\n",
    "print(synoms[0].lemmas()[0].name())\n",
    "\n",
    "# Definition of Revoke\n",
    "print(synoms[0].definition())\n",
    "\n",
    "# Context or example\n",
    "print(synoms[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a program to store synonyms and antonyms for a word 'Active'\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"Active\"):\n",
    "    for l in syn.lemmas():\n",
    "        print(\"lemmas :\", l)\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "print(set(synonyms))\n",
    "print(set(antonyms))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compare the symantic similarity for both words\n",
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"boat.n.01\")\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"car.n.01\")\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"cat.n.01\")\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"cactus.n.01\")\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1 = wordnet.synset(\"cab.n.01\")\n",
    "w2 = wordnet.synset(\"taxi.n.01\")\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Text Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        documents.append(list(movie_reviews.words(fileid)))\n",
    "random.shuffle(documents) \n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())            # Normalize all the words, make them lower case\n",
    "\n",
    "# Remove stopwords\n",
    "stopword = ['the','.','a','and','of','to',\"'\",'s','-','it','in','\"','r','(',')']\n",
    "fltoutwords = []\n",
    "for s in all_words:\n",
    "    if s not in stopword:\n",
    "        fltoutwords.append(s)\n",
    "        \n",
    "\n",
    "# To find most frequently used words\n",
    "fltoutwords = nltk.FreqDist(fltoutwords)\n",
    "print(fltoutwords.most_common(15))           # most commonly use 15 words out of all words\n",
    "\n",
    "# Count how many times use 'Stupid' word\n",
    "print(fltoutwords[\"stupid\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Words as a feature for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the movie_reviews documents from nltk package\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Check types of category are present in this movie review document\n",
    "category = movie_reviews.categories()\n",
    "print(category)\n",
    "\n",
    "# Check number of fileid are present in the documents\n",
    "filedid = movie_reviews.fileids()\n",
    "#print(filedid)\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "        \n",
    "# print(documents[0])\n",
    "random.shuffle(documents) \n",
    "\n",
    "# Store all words in a list in lower case\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())   # We are takeing all words from all documents, then normalized them and then make them lower case\n",
    "\n",
    "# Findout most frequently happening words\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "#print((list(all_words))[:20])    # Print first 20 words\n",
    "\n",
    "# Now pickup top 3000 words from the most frequently happening words and store into a list\n",
    "mosthappend = list(all_words.keys())[:3000]        # Frequently happening words treat as feature words\n",
    "# print(mosthappend)\n",
    "\n",
    "# Filtered out the stop words from the top 3000 words\n",
    "stopword = ['the','.','a','and','of','to',\"'\",'s','-','it','in','\"','r','(',')',':',',']\n",
    "featurewords = []\n",
    "for s in mosthappend:\n",
    "    if s not in stopword:\n",
    "        featurewords.append(s)\n",
    "        \n",
    "# Write a function to find the feature word and key\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}        # Empty dictionary\n",
    "    \n",
    "    for w in featurewords:\n",
    "        features[w] = (w in words)\n",
    "        \n",
    "    return features\n",
    "print((find_features(movie_reviews.words('pos/cv001_18431.txt'))))\n",
    "\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "#print(len(featuresets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 13. Naive Bayes : text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = featuresets[:1900]\n",
    "testing_set = featuresets[1900:]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)     # train the model with training_set data\n",
    "print(\"Classifier accuracy percentage :\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Save Classifier with Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_classifier = open(\"C:/PersonalWork/DataAnalysis/Data/Allpickles/naivebayes.pickle\", 'wb')\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can use this pickle for later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_f = open(\"C:/PersonalWork/DataAnalysis/Data/Allpickles/naivebayes.pickle\",'rb')\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()\n",
    "print(\" Original Classifier accuracy percentage :\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Scikit-Learn classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC,NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use MultinomialNB classifies\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percentage :\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BNB_classifier.train(training_set)\n",
    "print(\"BNB_classifier accuracy percentage :\", (nltk.classify.accuracy(BNB_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Logs_classifier = SklearnClassifier(LogisticRegression())\n",
    "Logs_classifier.train(training_set)\n",
    "print(\"Logs_classifier accuracy percentage :\", (nltk.classify.accuracy(Logs_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SGD_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGD_classifier.train(training_set)\n",
    "print(\"SGD_classifier accuracy percentage :\", (nltk.classify.accuracy(SGD_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "print(\"SVC_classifier accuracy percentage :\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Linr_classifier = SklearnClassifier(LinearSVC())\n",
    "Linr_classifier.train(training_set)\n",
    "print(\"Liner_classifier accuracy percentage :\", (nltk.classify.accuracy(Linr_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NUSVC_classifier accuracy percentage :\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Combining Algo with a Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):           # We are calling our class VoteClassifier\n",
    "        self._classifiers = classifiers         # We are assigning list of classifiers that are passed to our class to self._classifiers\n",
    "        \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)      # Based on feature we classify, each classification we treated as vote\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)      # Based on feature we classify, each classification we treated as vote\n",
    "            votes.append(v)\n",
    "            \n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        confnd = choice_votes / len(votes)\n",
    "        \n",
    "        return confnd\n",
    "\n",
    "# We are prepare a list of document id and their category\n",
    "documents = [(list(movie_reviews.words(fileIds)), category)\n",
    "            for category in movie_reviews.categories()\n",
    "            for fileIds in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)  # Shuffle them to avoid negative and positive bunch\n",
    "\n",
    "# Store all the words from the documents and make them lower\n",
    "allwords = []\n",
    "\n",
    "for w in movie_reviews.words():\n",
    "    allwords.append(w.lower())\n",
    "    \n",
    "# Pick up most freaquently using word \n",
    "\n",
    "allwords = nltk.FreqDist(allwords)\n",
    "\n",
    "# Now pick up top 3000 words from the most frequently using words (lets say : pick up top 3000 words)\n",
    "\n",
    "word_features = list(allwords.keys())[:3000]\n",
    "\n",
    "# build a function which find these 3000 words are present or not in our negative and positive documents(all documents) and mark them\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}        # Empty dictionary\n",
    "    \n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features\n",
    "\n",
    "# we can print one feature set like :\n",
    "# print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))\n",
    "\n",
    "# We can save feature existance booleans and their respective category(negative or positive)\n",
    "\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "    \n",
    "# Now split the words into train set and test set\n",
    "\n",
    "trainset = featuresets[:1900]\n",
    "testset = featuresets[1900:]\n",
    "\n",
    "# Using naive bayes classifier model \n",
    "# classifier = nltk.NaiveBayesClassifier.train(trainset)\n",
    "#print(\"Origina Naivebayes accuracy % : \", (nltk.classify.accuracy(classifier, testset)))\n",
    "#classifier.show_most_informative_features(10)\n",
    "\n",
    "\n",
    "# Using Naivebayes Model pickled \n",
    "classifier_f = open(\"C:/PersonalWork/DataAnalysis/Data/Allpickles/naivebayes.pickle\", 'rb')\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()\n",
    "print(\" Original Classifier accuracy percentage :\", (nltk.classify.accuracy(classifier, testset))*100)\n",
    "classifier.show_most_informative_features(10)\n",
    "\n",
    "# Use Multinomial Naive Bayes Model\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(trainset)\n",
    "print(\" MultinomialNB_classifier accuracy percentage :\", (nltk.classify.accuracy(MNB_classifier, testset))*100)\n",
    "\n",
    "\n",
    "BNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BNB_classifier.train(trainset)\n",
    "print(\" BournouliNB_classifier accuracy percentage :\", (nltk.classify.accuracy(BNB_classifier, testset))*100)\n",
    "\n",
    "Logs_classifier = SklearnClassifier(LogisticRegression())\n",
    "Logs_classifier.train(trainset)\n",
    "print(\" LogisticsRegre_classifier accuracy percentage :\", (nltk.classify.accuracy(Logs_classifier, testset))*100)\n",
    "\n",
    "\n",
    "SGD_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGD_classifier.train(trainset)\n",
    "print(\"Stochastics_Gradient_Distance accuracy percentage :\", (nltk.classify.accuracy(SGD_classifier, testset))*100)\n",
    "\n",
    "\n",
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(trainset)\n",
    "print(\"Support Vector_classifier accuracy percentage :\", (nltk.classify.accuracy(SVC_classifier, testset))*100)\n",
    "\n",
    "\n",
    "Linr_classifier = SklearnClassifier(LinearSVC())\n",
    "Linr_classifier.train(trainset)\n",
    "print(\" Liner_classifier accuracy percentage :\", (nltk.classify.accuracy(Linr_classifier, testset))*100)\n",
    "            \n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(trainset)\n",
    "print(\"Number Support Vector_classifier accuracy percentage :\", (nltk.classify.accuracy(NuSVC_classifier, testset))*100)\n",
    "\n",
    "\n",
    "voted_classifier = VoteClassifier(classifier,\n",
    "                                 MNB_classifier,\n",
    "                                 BNB_classifier,\n",
    "                                 Logs_classifier,\n",
    "                                 SVC_classifier,\n",
    "                                 Linr_classifier,\n",
    "                                 NuSVC_classifier)\n",
    "\n",
    "print(\"Voted Classifier accuracy percentage :\", (nltk.classify.accuracy(voted_classifier, testset))*100)\n",
    "#print(\"Voted Classifier accuracy % :\",(nltk.classify.accuracy(voted_classifier, testset))* 100)\n",
    "\n",
    "print(\"Classification:\", voted_classifier.classify(testset[0][0]), \"Confidence %:\",voted_classifier.confidence(testset[0][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testset[1][0]), \"Confidence %:\",voted_classifier.confidence(testset[1][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testset[2][0]), \"Confidence %:\",voted_classifier.confidence(testset[2][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testset[3][0]), \"Confidence %:\",voted_classifier.confidence(testset[3][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testset[4][0]), \"Confidence %:\",voted_classifier.confidence(testset[4][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testset[5][0]), \"Confidence %:\",voted_classifier.confidence(testset[5][0])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Improving Training data for sentiment Analysis\n",
    "\n",
    "we can build our new dataset in very similar way as before. We need a new methodology for creating new 'documents' variable and new way to create 'allwords' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origina Naivebayes accuracy % :  71.3855421686747\n",
      "Most Informative Features\n",
      "                    warm = True              pos : neg    =     22.5 : 1.0\n",
      "              engrossing = True              pos : neg    =     20.5 : 1.0\n",
      "               inventive = True              pos : neg    =     15.8 : 1.0\n",
      "                supposed = True              neg : pos    =     14.9 : 1.0\n",
      "               absorbing = True              pos : neg    =     13.8 : 1.0\n",
      "                   vivid = True              pos : neg    =     12.4 : 1.0\n",
      "               wonderful = True              pos : neg    =     11.9 : 1.0\n",
      "            refreshingly = True              pos : neg    =     11.7 : 1.0\n",
      "                captures = True              pos : neg    =     11.5 : 1.0\n",
      "                  stupid = True              neg : pos    =     11.3 : 1.0\n",
      " MultinomialNB_classifier accuracy percentage : 71.6867469879518\n",
      " BournouliNB_classifier accuracy percentage : 71.53614457831326\n",
      " LogisticsRegre_classifier accuracy percentage : 72.89156626506023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IBM_ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastics_Gradient_Distance accuracy percentage : 67.62048192771084\n",
      "Support Vector_classifier accuracy percentage : 47.43975903614458\n",
      " Liner_classifier accuracy percentage : 69.57831325301204\n",
      "Number Support Vector_classifier accuracy percentage : 71.98795180722891\n",
      "Voted Classifier accuracy percentage : 71.53614457831326\n",
      "Classification: neg Confidence %: 100.0\n",
      "Classification: pos Confidence %: 85.71428571428571\n",
      "Classification: pos Confidence %: 71.42857142857143\n",
      "Classification: pos Confidence %: 57.14285714285714\n",
      "Classification: neg Confidence %: 71.42857142857143\n",
      "Classification: neg Confidence %: 100.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "import random\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):           # We are calling our class VoteClassifier\n",
    "        self._classifiers = classifiers         # We are assigning list of classifiers that are passed to our class to self._classifiers\n",
    "        \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)      # Based on feature we classify, each classification we treated as vote\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)      # Based on feature we classify, each classification we treated as vote\n",
    "            votes.append(v)\n",
    "            \n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        confnd = choice_votes / len(votes)\n",
    "        \n",
    "        return confnd\n",
    "\n",
    "\n",
    "short_pos = open(\"C:/PersonalWork/DataAnalysis/Data/TextAnalytics/positive.txt\", \"r\").read()\n",
    "short_neg = open(\"C:/PersonalWork/DataAnalysis/Data/TextAnalytics/negative.txt\", \"r\").read()\n",
    "\n",
    "# Creating documents to store document and their category\n",
    "documents = []\n",
    "\n",
    "for r in short_pos.split('\\n'):\n",
    "    documents.append((r, 'pos'))\n",
    "    \n",
    "for r in short_neg.split('\\n'):\n",
    "    documents.append((r, 'neg'))\n",
    "    \n",
    "allWords = []\n",
    "\n",
    "short_pos_word = word_tokenize(short_pos)\n",
    "short_neg_word = word_tokenize(short_neg)\n",
    "\n",
    "for w in short_pos_word:\n",
    "    allWords.append(w.lower())\n",
    "    \n",
    "for w in short_neg_word:\n",
    "    allWords.append(w.lower())\n",
    "\n",
    "# Most Frequently using word\n",
    "\n",
    "allWords = nltk.FreqDist(allWords)\n",
    "\n",
    "# Pickup top 5000 words\n",
    "word_features = list(allWords.keys())[:5000]\n",
    "\n",
    "# Build find features function\n",
    "\n",
    "def findFeatures(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    \n",
    "    for f in word_features:\n",
    "        features[f] = (f in words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(findFeatures(rev), category) for (rev, category) in documents]\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "# Positive Data example\n",
    "trainSet = featuresets[:10000]\n",
    "testSet = featuresets[10000:]\n",
    "\n",
    "# Using naive bayes classifier model \n",
    "classifier = nltk.NaiveBayesClassifier.train(trainSet)\n",
    "print(\"Origina Naivebayes accuracy % : \", (nltk.classify.accuracy(classifier, testSet))*100)\n",
    "classifier.show_most_informative_features(10)\n",
    "\n",
    "# Use Multinomial Naive Bayes Model\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(trainSet)\n",
    "print(\" MultinomialNB_classifier accuracy percentage :\", (nltk.classify.accuracy(MNB_classifier, testSet))*100)\n",
    "\n",
    "\n",
    "BNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BNB_classifier.train(trainSet)\n",
    "print(\" BournouliNB_classifier accuracy percentage :\", (nltk.classify.accuracy(BNB_classifier, testSet))*100)\n",
    "\n",
    "Logs_classifier = SklearnClassifier(LogisticRegression())\n",
    "Logs_classifier.train(trainSet)\n",
    "print(\" LogisticsRegre_classifier accuracy percentage :\", (nltk.classify.accuracy(Logs_classifier, testSet))*100)\n",
    "\n",
    "\n",
    "SGD_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGD_classifier.train(trainSet)\n",
    "print(\"Stochastics_Gradient_Distance accuracy percentage :\", (nltk.classify.accuracy(SGD_classifier, testSet))*100)\n",
    "\n",
    "\n",
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(trainSet)\n",
    "print(\"Support Vector_classifier accuracy percentage :\", (nltk.classify.accuracy(SVC_classifier, testSet))*100)\n",
    "\n",
    "\n",
    "Linr_classifier = SklearnClassifier(LinearSVC())\n",
    "Linr_classifier.train(trainSet)\n",
    "print(\" Liner_classifier accuracy percentage :\", (nltk.classify.accuracy(Linr_classifier, testSet))*100)\n",
    "            \n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(trainSet)\n",
    "print(\"Number Support Vector_classifier accuracy percentage :\", (nltk.classify.accuracy(NuSVC_classifier, testSet))*100)\n",
    "\n",
    "\n",
    "voted_classifier = VoteClassifier(classifier,\n",
    "                                 MNB_classifier,\n",
    "                                 BNB_classifier,\n",
    "                                 Logs_classifier,\n",
    "                                 SVC_classifier,\n",
    "                                 Linr_classifier,\n",
    "                                 NuSVC_classifier)\n",
    "\n",
    "print(\"Voted Classifier accuracy percentage :\", (nltk.classify.accuracy(voted_classifier, testSet))*100)\n",
    "#print(\"Voted Classifier accuracy % :\",(nltk.classify.accuracy(voted_classifier, testset))* 100)\n",
    "\n",
    "print(\"Classification:\", voted_classifier.classify(testSet[0][0]), \"Confidence %:\",voted_classifier.confidence(testSet[0][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testSet[1][0]), \"Confidence %:\",voted_classifier.confidence(testSet[1][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testSet[2][0]), \"Confidence %:\",voted_classifier.confidence(testSet[2][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testSet[3][0]), \"Confidence %:\",voted_classifier.confidence(testSet[3][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testSet[4][0]), \"Confidence %:\",voted_classifier.confidence(testSet[4][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testSet[5][0]), \"Confidence %:\",voted_classifier.confidence(testSet[5][0])*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 19. Creating a Module for sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10664\n",
      "Original naive Bayes Classifier accuracy  55.12048192771084\n",
      "Most Informative Features\n",
      "                       < = True              pos : neg    =      2.3 : 1.0\n",
      "                       é = True              neg : pos    =      1.9 : 1.0\n",
      "                       i = False             neg : pos    =      1.6 : 1.0\n",
      "                       * = True              pos : neg    =      1.6 : 1.0\n",
      "                       ‘ = True              pos : neg    =      1.4 : 1.0\n",
      "                       f = False             neg : pos    =      1.4 : 1.0\n",
      "                       g = False             neg : pos    =      1.2 : 1.0\n",
      "                       o = False             pos : neg    =      1.2 : 1.0\n",
      "                       m = False             neg : pos    =      1.2 : 1.0\n",
      "                       h = False             pos : neg    =      1.2 : 1.0\n",
      "                       n = False             neg : pos    =      1.2 : 1.0\n",
      "                       t = False             neg : pos    =      1.1 : 1.0\n",
      "                       l = False             neg : pos    =      1.1 : 1.0\n",
      "                       q = True              pos : neg    =      1.1 : 1.0\n",
      "                       – = True              neg : pos    =      1.1 : 1.0\n",
      "Multinomial Naive bayes Claasifer Accuracy  54.06626506024096\n",
      "Bernoulli Naive bayes Claasifer Accuracy  54.81927710843374\n",
      "Logistics Regression Claasifer Accuracy  55.27108433734939\n",
      "Linear SVC Claasifer Accuracy  55.27108433734939\n",
      "Numeric Support Vector Claasifer Accuracy  50.0\n",
      "Support vector Claasifer Accuracy  49.39759036144578\n",
      "Voted Classifier accuracy : 55.12048192771084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('pos', 0.8571428571428571)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "import random\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Define a class for vote classifier\n",
    "\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):         # Call VoteClassifier class it self, and passing self and classifiers parameter\n",
    "        self._classifiers = classifiers       # We are assigning the list of classifiers \n",
    "        \n",
    "    # Here we will counting the votes, Classify(Pos or Neg) the based on features(words)\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "    \n",
    "    # Counting the confidence level (gotten votes / Total votes) \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "            \n",
    "        votes_choose = votes.count(mode(votes))\n",
    "        confd = votes_choose / len(votes)\n",
    "        return confd\n",
    "    \n",
    "shortpos = open(\"C:/PersonalWork/DataAnalysis/Data/TextAnalytics/positive.txt\", \"r\").read()\n",
    "shortneg = open(\"C:/PersonalWork/DataAnalysis/Data/TextAnalytics/negative.txt\", \"r\").read()\n",
    "\n",
    "# Step 1 : make a documents\n",
    "# Step 2 : tokenized words\n",
    "# step 3 : Most Frequently happening words\n",
    "# Step 4 : Pick up top 5000 words, that would would be treating as features\n",
    "# Step 5 : Define List of stop words\n",
    "# Step 6 : Filterout stopwords\n",
    "# Step 7 : tag each word with part-of-speech\n",
    "# Step 8 : Allowed part-of-speech words \n",
    "# Step 9 : Create a function to findout the feature in each documents\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "documents = []\n",
    "allwords = []\n",
    "# Step 1 : Make a document list\n",
    "for r in shortpos.split('\\n'):\n",
    "    documents.append((r, \"pos\"))\n",
    "\n",
    "for r in shortneg.split('\\n'):\n",
    "    documents.append((r, \"neg\"))\n",
    "\n",
    "#print(documents)\n",
    "# Tokenized the words \n",
    "shortpos_words = word_tokenize(shortpos)\n",
    "shortneg_words = word_tokenize(shortneg)\n",
    "\n",
    "# Step 2:  Make allwords list \n",
    "for w in shortpos_words:\n",
    "    allwords.append(w.lower())\n",
    "for w in shortneg_words:\n",
    "    allwords.append(w.lower())\n",
    "    \n",
    "\n",
    "# Step 5 : Define list of stop words \n",
    "stopwords = ['the','.','a','and','of','to',\"'\",'s','-','it','in','\"','r','(',')', 'b']\n",
    "\n",
    "# Step 6 : Removed stop words\n",
    "fltoutwords = []\n",
    "for f in allwords:\n",
    "    if f not in stopwords:\n",
    "        fltoutwords.append(f)\n",
    "# print(\"Filtered out stopwords :\", fltoutwords)\n",
    "# Step 7 : Tagged POS\n",
    "pos = nltk.pos_tag(fltoutwords)\n",
    "\n",
    "print(\"Part-Of-Speech :\", pos)\n",
    "# Define allowed part-of-speech : Adjective : 'JJ', Verb : 'VB', and Adverb :'RB', JJR\n",
    "allowed_pos_words = ['J']\n",
    "final_feature_words = []\n",
    "for f in pos:\n",
    "    if f[1][0] == 'J':\n",
    "        final_feature_words.append(f[0])\n",
    "\n",
    "print(\"Final Feature Words \", final_feature_words)\n",
    "# Step 9 : Find out features from each document\n",
    "\n",
    "def findfeatures(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for f in final_feature_words:\n",
    "        features[f] = (f in words)\n",
    "    return features\n",
    "\n",
    "# Prepared the features set from all the documents\n",
    "featuresets = [(findfeatures(rev), category) for (rev, category) in documents]\n",
    "\n",
    "random.shuffle(featuresets)\n",
    "print(len(featuresets))\n",
    "\n",
    "training_set = featuresets[:10000]\n",
    "testing_set = featuresets[10000:]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Original naive Bayes Classifier accuracy \", (nltk.classify.accuracy(classifier, testing_set)*100))\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"Multinomial Naive bayes Claasifer Accuracy \", (nltk.classify.accuracy(MNB_classifier, testing_set)*100))\n",
    "\n",
    "BNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BNB_classifier.train(training_set)\n",
    "print(\"Bernoulli Naive bayes Claasifer Accuracy \", (nltk.classify.accuracy(BNB_classifier, testing_set)*100))\n",
    "\n",
    "LogR_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogR_classifier.train(training_set)\n",
    "print(\"Logistics Regression Claasifer Accuracy \", (nltk.classify.accuracy(LogR_classifier, testing_set)*100))\n",
    "\n",
    "Linr_classifier = SklearnClassifier(LinearSVC())\n",
    "Linr_classifier.train(training_set)\n",
    "print(\"Linear SVC Claasifer Accuracy \", (nltk.classify.accuracy(Linr_classifier, testing_set)*100))\n",
    "\n",
    "NuS_classifier = SklearnClassifier(NuSVC())\n",
    "NuS_classifier.train(training_set)\n",
    "print(\"Numeric Support Vector Claasifer Accuracy \", (nltk.classify.accuracy(NuS_classifier, testing_set)*100))\n",
    "Svc_classifier = SklearnClassifier(SVC())\n",
    "Svc_classifier.train(training_set)\n",
    "print(\"Support vector Claasifer Accuracy \", (nltk.classify.accuracy(Svc_classifier, testing_set)*100))\n",
    "\n",
    "votedClassifier = VoteClassifier(classifier,\n",
    "                                MNB_classifier,\n",
    "                                BNB_classifier,\n",
    "                                LogR_classifier,\n",
    "                                Linr_classifier,\n",
    "                                NuS_classifier,\n",
    "                                Svc_classifier)\n",
    "\n",
    "print(\"Voted Classifier accuracy :\",(nltk.classify.accuracy(votedClassifier, testing_set)*100))\n",
    "\n",
    "text = \"This movie was awesome! The acting was great, plot was wonderful, and there were pythons...so yea!\"\n",
    "\n",
    "def sentiment():\n",
    "    feats = findfeatures(text)\n",
    "    return votedClassifier.classify(feats), votedClassifier.confidence(feats)\n",
    "sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "sid = SIA()\n",
    "print(sid.polarity_scores(\"This movie was ok, but not awasome.\\n\"))\n",
    "print(sid.polarity_scores(\"This is not a good time for job change.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
